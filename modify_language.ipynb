{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhekai/miniforge3/envs/vllm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoProcessor, AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import textwrap\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import enum\n",
    "import json\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-24 15:12:00 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 15:12:00,385\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-24 15:12:04 config.py:528] This model supports multiple tasks: {'score', 'reward', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 01-24 15:12:04 llm_engine.py:232] Initializing an LLM engine (v0.1.dev4262+g6609cdf) with config: model='logs/llama-bridge', speculative_config=None, tokenizer='logs/llama-bridge', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=logs/llama-bridge, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-24 15:12:04 cuda.py:225] Using Flash Attention backend.\n",
      "INFO 01-24 15:12:05 model_runner.py:1110] Starting to load model logs/llama-bridge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:00,  2.51it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:00<00:00,  2.00it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  1.86it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-24 15:12:06 model_runner.py:1115] Loading model weights took 12.5527 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-24 15:12:07 worker.py:266] Memory profiling takes 0.76 seconds\n",
      "INFO 01-24 15:12:07 worker.py:266] the current vLLM instance can use total_gpu_memory (23.64GiB) x gpu_memory_utilization (0.80) = 18.91GiB\n",
      "INFO 01-24 15:12:07 worker.py:266] model weights take 12.55GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 0.44GiB; the rest of the memory reserved for KV Cache is 5.84GiB.\n",
      "INFO 01-24 15:12:07 executor_base.py:107] # CUDA blocks: 748, # CPU blocks: 512\n",
      "INFO 01-24 15:12:07 executor_base.py:112] Maximum concurrency for 2048 tokens per request: 5.84x\n",
      "INFO 01-24 15:12:09 model_runner.py:1448] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:10<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-24 15:12:19 model_runner.py:1574] Graph capturing finished in 11 secs, took 0.24 GiB\n",
      "INFO 01-24 15:12:19 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 12.90 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "language_model = LLM(\"logs/llama-bridge\",  trust_remote_code=True, gpu_memory_utilization=0.8, device=\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "import torch\n",
    "input_ids = [1] * 317\n",
    "sampling_params = SamplingParams(temperature=0.0,\n",
    "                                     max_tokens=300,\n",
    "                                     stop_token_ids= [2])\n",
    "# outputs = language_model.generate(prompt_token_ids=input_ids, sampling_params=sampling_params)\n",
    "multimodal_embeddings = torch.load(\"logs/multimodal_embeddings_cpu.pt\", map_location=\"cpu\", weights_only=True).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 32000], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"logs/llama-bridge\")\n",
    "tokenizer(\"<PAD>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.58s/it, est. speed input: 69.19 toks/s, output: 62.64 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs  = language_model.generate({\"prompt_token_ids\": input_ids, \"multi_modal_data\": {\"image\": multimodal_embeddings}}, sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15484, 278, 16699, 837, 295, 265, 373, 278, 304, 20466, 29889, 16507, 2190, 29901, 25249, 278, 5075, 304, 278, 1492, 322, 701, 29892, 322, 2058, 278, 16699, 837, 295, 265, 373, 278, 304, 20466, 29889, 478, 3235, 8979, 1307, 438, 29933, 17637, 29903, 29901, 263, 13328, 805, 6150, 518, 29906, 29906, 29892, 29871, 29896, 29906, 29929, 29892, 29871, 29947, 29900, 29892, 29871, 29896, 29945, 29906, 1402, 278, 9088, 518, 29896, 29896, 29892, 29871, 29896, 29892, 29871, 29906, 29946, 29896, 29892, 29871, 29906, 29945, 29900, 1402, 263, 304, 29891, 330, 3055, 17615, 518, 29896, 29896, 29900, 29892, 29871, 29896, 29945, 29945, 29892, 29871, 29896, 29953, 29900, 29892, 29871, 29906, 29906, 29929, 1402, 263, 7254, 13950, 518, 29896, 29945, 29945, 29892, 29871, 29929, 29953, 29892, 29871, 29906, 29906, 29955, 29892, 29871, 29896, 29953, 29946, 29962, 27092, 29911, 3289, 29968, 5195, 29909, 3094, 4214, 29901, 450, 330, 374, 2496, 4225, 304, 4337, 304, 278, 1492, 322, 701, 297, 1797, 304, 2058, 278, 16699, 837, 295, 265, 373, 278, 304, 20466, 29889, 27092, 29911, 3289, 29968, 29901, 25249, 278, 5075, 304, 278, 1492, 322, 701, 29889, 16999, 12064, 5195, 29909, 3094, 4214, 29901, 450, 330, 374, 2496, 4225, 304, 4337, 304, 278, 1492, 322, 701, 297, 1797, 304, 2058, 278, 16699, 837, 295, 265, 373, 278, 304, 20466, 29889, 16999, 12064, 29901, 25249, 1492, 701, 29889, 402, 3960, 29925, 13171, 349, 3267, 22122, 29901, 518, 29896, 29896, 29929, 29892, 29871, 29929, 29900, 29892, 29871, 29896, 29941, 29947, 29892, 29871, 29947, 29946, 29892, 29871, 29896, 29945, 29946, 29892, 29871, 29947, 29900, 29892, 29871, 29896, 29953, 29929, 29892, 29871, 29947, 29900, 29892, 29871, 29896, 29947, 29941, 29892, 29871, 29947, 29945, 29962, 319, 9838, 29901, 29871, 31831, 31978, 31824, 31826, 31931, 31912, 31872, 2)\n",
      "287\n"
     ]
    }
   ],
   "source": [
    "for o in outputs:\n",
    "    generated_text = o.outputs[0].token_ids\n",
    "    print(generated_text)\n",
    "    print(len(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Place the watermelon on the towel. PLAN: Move the arm to the right and up, and place the watermelon on the towel. VISIBLE OBJECTS: a yellow spoon [22, 129, 80, 152], the scene [11, 1, 241, 250], a toy giraffe [110, 155, 160, 229], a blue cloth [155, 96, 227, 164] SUBTASK REASONING: The gripper needs to move to the right and up in order to place the watermelon on the towel. SUBTASK: Move the arm to the right and up. MOVE REASONING: The gripper needs to move to the right and up in order to place the watermelon on the towel. MOVE: Move right up. GRIPPER POSITION: [119, 90, 138, 84, 154, 80, 169, 80, 183, 85] ACTION: 塔ფ斯만助昭Ÿ'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.outputs[0].text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
